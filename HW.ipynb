{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787b73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import tiktoken\n",
    "import random\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a361b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed so we get the same results\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee72b2",
   "metadata": {},
   "source": [
    "First, we need read the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3c6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poetry.csv', newline='') as csvfile:\n",
    "    raw_data = list(csv.reader(csvfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c46c73",
   "metadata": {},
   "source": [
    "Then, we process it into a form that our GPT model can use. This is where we have to design our prompt and use soft prompting. Fill in the function `process_poem` to return a prompt based on the information we have about the poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e99b2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poem(author, name, age, poem_type):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        author: str\n",
    "            The author of the poem\n",
    "        name: str\n",
    "            The name of the poem\n",
    "        age: str\n",
    "            The \"age\" that the poem is from (either \"Renaissance\" or \"Modern\")\n",
    "                Note: each poem author will either be in the Renaissance\n",
    "                      or Modern age, meaning that this is redundant information\n",
    "        poem_type: str\n",
    "            The type of poem, will be one of these: \"Love\", \"Mythology & Folklore\", \"Nature\"\n",
    "    \n",
    "    Returns:\n",
    "        prompt: str\n",
    "            The prompt that we use for soft prompting\n",
    "    \"\"\"\n",
    "    \n",
    "    return \"Here is a \" + age + \" \" + poem_type + \" poem written by \" + author + ' called \"' + name + '\"\\n\\n'\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "encode = lambda x: enc.encode_ordinary(x)\n",
    "\n",
    "dataset = [(encode(process_poem(author, name, age, poem_type)), \n",
    "            encode(poem_content)) \n",
    "           for author, poem_content, name, age, poem_type in raw_data[1:]]\n",
    "\n",
    "max_prompt_length = max([len(poem[0]) for poem in dataset])\n",
    "max_poem_length   = max([len(poem[1]) for poem in dataset])\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "n = len(dataset)\n",
    "train_data = dataset[:int(n*0.9)]\n",
    "val_data = dataset[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a706ce",
   "metadata": {},
   "source": [
    "Next, we need to write code to sample from our dataset and generate a batch of data for us to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e309c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: List[prompt, poem_content]\n",
    "            The dataset that we want to sample from (either training or validation)\n",
    "    Returns:\n",
    "        x: np.array[shape=(batch_size, datapoint_length)]\n",
    "        y: np.array[shape=(batch_size, datapoint_length)]\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_index = random.randrange(len(data))\n",
    "    \n",
    "    concat_data = np.array(data[sample_index][0] + data[sample_index][1])\n",
    "    \n",
    "    block_size = min(1025, len(concat_data))\n",
    "    \n",
    "    x = np.zeros(shape=(batch_size, block_size-1), dtype=np.int64)\n",
    "    y = np.zeros(shape=(batch_size, block_size-1), dtype=np.int64)\n",
    "\n",
    "    x = concat_data[:block_size-1]\n",
    "    y = concat_data[1:block_size]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e042b4",
   "metadata": {},
   "source": [
    "## Now, we train the model\n",
    "\n",
    "First things first, we set some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e960b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out'\n",
    "log_interval = 1\n",
    "eval_interval = 8\n",
    "eval_iters = 20\n",
    "\n",
    "batch_size = 64\n",
    "dropout = 0.0\n",
    "\n",
    "learning_rate = 2e-4 # max learning rate\n",
    "max_iters = 64 # total number of training iterations\n",
    "warmup_iters = 4\n",
    "lr_decay_iters = 32\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "min_lr = 2e-6\n",
    "\n",
    "grad_clip = 1.0\n",
    "device = \"cuda\"\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dc34f",
   "metadata": {},
   "source": [
    "Then we load the GPT2-medium model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2699e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights\n",
      "loading weights from pretrained gpt: gpt2-medium\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahar/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 353.77M\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing from OpenAI GPT-2 weights\")\n",
    "\n",
    "override_args = dict(dropout=dropout)\n",
    "model = GPT.from_pretrained(\"gpt2-medium\", override_args)\n",
    "model_args = {}\n",
    "for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "    model_args[k] = getattr(model.config, k)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), \"cuda\")\n",
    "ctx = torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8173516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(author, name, age, poem_type):\n",
    "    start_ids = encode(process_poem(author, name, age, poem_type))\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            y = model.generate(x, 256, temperature=0.8, top_k=200)\n",
    "            return enc.decode(y[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1b655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Renaissance Nature poem written by HENRY VIII, KING OF ENGLAND called \"Midnight sun\"\n",
      "\n",
      "\n",
      "\"At sea we still go up alone\n",
      "\n",
      "He will do this again another time.\"\n",
      "\n",
      "\n",
      "A couple of things have happened to this poem. Firstly, the poem made a lot of noise during the 17th century. The English Sea Age and the influence of French Renaissance poets such as Richard Berry and Jonathan Swift have to be acknowledged here. Secondly, the English poem wasn't just done for the entertainment of the public (much to the chagrin of the French and Spanish Royalty who wanted to keep the poem an exclusive secret). It's also interesting to note that this English version of the poem was not really an English adaptation but, in fact, a very early French translation of the poem (perhaps a mixture of the two versions?).\n",
      "\n",
      "By the mid 14th century, the English language was well established and the word \"sun\" had entered common usage in English. Also, many of the English poets of that time, such as Edward Joyce, D.H. Lawrence and George Eliot had a very fondness for poetry, which translates well to the English language (it's hard to argue that D.H. Lawrence was 100% Irish). More importantly though, is that the English language was being used by many famous writers, such as William Shakespeare,\n"
     ]
    }
   ],
   "source": [
    "print(sample_model(\"HENRY VIII, KING OF ENGLAND\", \"Midnight sun\", \"Renaissance\", \"Nature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624f3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            the_data = train_data if split == 'train' else val_data\n",
    "            X, Y = get_batch(the_data)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def get_batch(data):\n",
    "    x, y = generate_batch(data)\n",
    "    x_torch = torch.from_numpy(x).pin_memory().to(device, non_blocking=True)[None, ...]\n",
    "    y_torch = torch.from_numpy(y).pin_memory().to(device, non_blocking=True)[None, ...]\n",
    "    return x_torch, y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58be150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.8747, val loss 4.8650\n",
      "iter 0: loss 4.2855, time 9520.89ms, mfu -100.00%\n",
      "iter 1: loss 4.8589, time 8356.17ms, mfu -100.00%\n",
      "iter 2: loss 4.8615, time 9769.59ms, mfu -100.00%\n",
      "iter 3: loss 4.8009, time 9520.53ms, mfu -100.00%\n",
      "iter 4: loss 4.3002, time 9142.24ms, mfu -100.00%\n",
      "iter 5: loss 4.4008, time 8654.19ms, mfu 5.88%\n",
      "iter 6: loss 4.0367, time 7772.94ms, mfu 5.95%\n",
      "iter 7: loss 4.5093, time 10931.83ms, mfu 5.82%\n",
      "step 8: train loss 3.2255, val loss 2.6474\n",
      "saving checkpoint to out\n",
      "iter 8: loss 3.4330, time 16657.02ms, mfu 5.55%\n",
      "iter 9: loss 1.6707, time 9833.04ms, mfu 5.51%\n",
      "iter 10: loss 3.7445, time 9974.26ms, mfu 5.47%\n",
      "iter 11: loss 2.8450, time 9377.84ms, mfu 5.47%\n",
      "iter 12: loss 1.4326, time 8583.87ms, mfu 5.51%\n",
      "iter 13: loss 3.3533, time 8462.71ms, mfu 5.56%\n",
      "iter 14: loss 3.3996, time 8139.78ms, mfu 5.63%\n",
      "iter 15: loss 3.4540, time 10817.62ms, mfu 5.54%\n",
      "step 16: train loss 2.7038, val loss 2.8791\n",
      "iter 16: loss 2.5836, time 10399.68ms, mfu 5.48%\n",
      "iter 17: loss 2.6575, time 8601.16ms, mfu 5.52%\n",
      "iter 18: loss 2.5561, time 8878.05ms, mfu 5.54%\n",
      "iter 19: loss 2.1918, time 9962.58ms, mfu 5.50%\n",
      "iter 20: loss 3.4795, time 9956.48ms, mfu 5.46%\n",
      "iter 21: loss 3.4623, time 9444.59ms, mfu 5.45%\n",
      "iter 22: loss 3.7406, time 9757.82ms, mfu 5.43%\n",
      "iter 23: loss 2.4477, time 9559.67ms, mfu 5.42%\n",
      "step 24: train loss 2.4869, val loss 2.9441\n",
      "iter 24: loss 3.2600, time 10730.45ms, mfu 5.35%\n",
      "iter 25: loss 2.4194, time 8634.51ms, mfu 5.41%\n",
      "iter 26: loss 3.0633, time 8789.93ms, mfu 5.45%\n",
      "iter 27: loss 2.9266, time 9033.03ms, mfu 5.47%\n",
      "iter 28: loss 2.8342, time 9817.72ms, mfu 5.44%\n",
      "iter 29: loss 0.3812, time 9737.61ms, mfu 5.42%\n",
      "iter 30: loss 0.3598, time 8182.63ms, mfu 5.50%\n",
      "iter 31: loss 2.7954, time 10434.37ms, mfu 5.44%\n",
      "step 32: train loss 2.2050, val loss 2.5616\n",
      "saving checkpoint to out\n",
      "iter 32: loss 3.1969, time 15427.70ms, mfu 5.22%\n",
      "iter 33: loss 3.1082, time 9522.54ms, mfu 5.23%\n",
      "iter 34: loss 0.4552, time 8634.59ms, mfu 5.30%\n",
      "iter 35: loss 2.8646, time 11187.83ms, mfu 5.23%\n",
      "iter 36: loss 3.5392, time 9824.51ms, mfu 5.22%\n",
      "iter 37: loss 1.4252, time 9537.04ms, mfu 5.23%\n",
      "iter 38: loss 2.8704, time 10296.75ms, mfu 5.21%\n",
      "iter 39: loss 2.4484, time 8594.54ms, mfu 5.28%\n",
      "step 40: train loss 2.7123, val loss 3.1286\n",
      "iter 40: loss 3.1311, time 11470.57ms, mfu 5.19%\n",
      "iter 41: loss 2.9456, time 9588.05ms, mfu 5.21%\n",
      "iter 42: loss 2.5029, time 10204.97ms, mfu 5.18%\n",
      "iter 43: loss 1.7927, time 11746.18ms, mfu 5.10%\n",
      "iter 44: loss 3.2871, time 10600.69ms, mfu 5.07%\n",
      "iter 45: loss 0.3611, time 8503.13ms, mfu 5.16%\n",
      "iter 46: loss 2.7918, time 11087.40ms, mfu 5.10%\n",
      "iter 47: loss 1.5936, time 8778.65ms, mfu 5.17%\n",
      "step 48: train loss 2.6425, val loss 2.8789\n",
      "iter 48: loss 2.9826, time 12563.43ms, mfu 5.06%\n",
      "iter 49: loss 3.3404, time 9235.23ms, mfu 5.11%\n",
      "iter 50: loss 2.3160, time 9292.83ms, mfu 5.14%\n",
      "iter 51: loss 0.2615, time 8429.85ms, mfu 5.23%\n",
      "iter 52: loss 0.3734, time 9408.67ms, mfu 5.25%\n",
      "iter 53: loss 0.6136, time 9941.74ms, mfu 5.24%\n",
      "iter 54: loss 0.7430, time 9011.48ms, mfu 5.28%\n",
      "iter 55: loss 3.0721, time 8574.06ms, mfu 5.35%\n",
      "step 56: train loss 2.6573, val loss 2.8709\n",
      "iter 56: loss 2.4683, time 10934.10ms, mfu 5.28%\n",
      "iter 57: loss 3.6041, time 7766.84ms, mfu 5.41%\n",
      "iter 58: loss 0.7138, time 11539.05ms, mfu 5.31%\n",
      "iter 59: loss 2.8168, time 10080.79ms, mfu 5.28%\n",
      "iter 60: loss 2.8905, time 8059.91ms, mfu 5.38%\n",
      "iter 61: loss 2.7568, time 9533.28ms, mfu 5.38%\n",
      "iter 62: loss 0.2778, time 8859.82ms, mfu 5.42%\n",
      "iter 63: loss 2.9146, time 10569.42ms, mfu 5.36%\n",
      "step 64: train loss 2.2507, val loss 2.9489\n",
      "iter 64: loss 0.2411, time 10775.15ms, mfu 5.29%\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "X, Y = get_batch(train_data)\n",
    "t0 = time.time()\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(batch_size):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / batch_size # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch(train_data)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * batch_size\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = model.estimate_mfu(batch_size, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "    \n",
    "model.eval()\n",
    "None # to avoid printing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d238b",
   "metadata": {},
   "source": [
    "Now we sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89cf181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Renaissance Nature poem written by HENRY VIII, KING OF ENGLAND called \"Midnight sun\"\n",
      "\n",
      "Hear me well, O moon, how oft it hath done my part to please,\r\n",
      "And now I rise in the dimness, and now that the sun hath moved,\r\n",
      "Unto me in the great shadow of the morning,\r\n",
      "And yet I hear the same loud voice, and yet I am turned after,\r\n",
      "And yet I hear the same deep sigh: but in that way,\r\n",
      "The night's shadow, that was my soul, Is but shadow, and must die.\r\n",
      "\r\n",
      "And now I hear the same smooth voice, in that calm sound of my heart,\r\n",
      "Which in deep sleep was but as a great storm,\r\n",
      "Unswayed by sound, since the night did get away.\r\n",
      "\r\n",
      "There is no night in this world but that which comes\r\n",
      "With suddenness, and that with pain,\r\n",
      "And that in rude season, and that when the day cannot be.\r\n",
      "\r\n",
      "And now I hear the same strong sigh, in that sound of my mind,\r\n",
      "That in my soul which was the storm, and that with pain,\r\n",
      "That in the storm still my self did break.\r\n",
      "\r\n",
      "And now I hear the same smooth voice\n"
     ]
    }
   ],
   "source": [
    "print(sample_model(\"HENRY VIII, KING OF ENGLAND\", \"Midnight sun\", \"Renaissance\", \"Nature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1497e5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Renaissance Nature poem written by HENRY VIII, KING OF ENGLAND called \"Midnight sun\"\n",
      "\n",
      "When noon is gone,\r\n",
      "And the noon beams rise,\r\n",
      "With strange eyes,\r\n",
      "Unhappy and mournful\r\n",
      "In the old twilight,\r\n",
      "    And they approach\r\n",
      "My golden chamber,\r\n",
      "    Which is the most full     Of light.\r\n",
      "\r\n",
      "Yet do they stir\r\n",
      "And the air moves,\r\n",
      "    Which might make me sleep.\r\n",
      "\r\n",
      "As they turn,\r\n",
      "And in a little way,\r\n",
      "Oh, they lie, and cry\r\n",
      "    For their king, for they mock\r\n",
      "    For their King to see:\r\n",
      "The best thing they can do,\r\n",
      "    For the world to look upon.\r\n",
      "They stare for their queen,\r\n",
      "    For love of love to have,\r\n",
      "    For the sun to shine,\r\n",
      "    And to have her hand\r\n",
      "    To hold.\r\n",
      "\r\n",
      "But they lie, and cry,\r\n",
      "    For their Queen and King\r\n",
      "    For love of love to have,\r\n",
      "    For the world to see.\r\n",
      "In love\n"
     ]
    }
   ],
   "source": [
    "print(sample_model(\"HENRY VIII, KING OF ENGLAND\", \"Midnight sun\", \"Renaissance\", \"Nature\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ceab59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
