{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787b73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import tiktoken\n",
    "import random\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a361b3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed so we get the same results\n",
    "\n",
    "seed = 1337\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee72b2",
   "metadata": {},
   "source": [
    "First, we need read the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3c6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poetry.csv', newline='') as csvfile:\n",
    "    raw_data = list(csv.reader(csvfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c46c73",
   "metadata": {},
   "source": [
    "Then, we process it into a form that our GPT model can use. This is where we have to design our prompt and use soft prompting. Fill in the function `process_poem` to return a prompt based on the information we have about the poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e99b2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poem(author, name, age, poem_type):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        author: str\n",
    "            The author of the poem\n",
    "        name: str\n",
    "            The name of the poem\n",
    "        age: str\n",
    "            The \"age\" that the poem is from (either \"Renaissance\" or \"Modern\")\n",
    "                Note: each poem author will either be in the Renaissance\n",
    "                      or Modern age, meaning that this is redundant information\n",
    "        poem_type: str\n",
    "            The type of poem, will be one of these: \"Love\", \"Mythology & Folklore\", \"Nature\"\n",
    "    \n",
    "    Returns:\n",
    "        prompt: str\n",
    "            The prompt that we use for soft prompting\n",
    "    \"\"\"\n",
    "    \n",
    "    return \"Here is a \" + age + \" \" + poem_type + \" poem written by \" + author + ' called \"' + name + '\"\\n\\n'\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "encode = lambda x: enc.encode_ordinary(x)\n",
    "\n",
    "dataset = [(encode(process_poem(author, name, age, poem_type)), \n",
    "            encode(poem_content)) \n",
    "           for author, poem_content, name, age, poem_type in raw_data[1:]]\n",
    "\n",
    "max_prompt_length = max([len(poem[0]) for poem in dataset])\n",
    "max_poem_length   = max([len(poem[1]) for poem in dataset])\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "n = len(dataset)\n",
    "train_data = dataset[:int(n*0.9)]\n",
    "val_data = dataset[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a706ce",
   "metadata": {},
   "source": [
    "Next, we need to write code to sample from our dataset and generate a batch of data for us to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e309c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data, batch_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: List[prompt, poem_content]\n",
    "            The dataset that we want to sample from (either training or validation)\n",
    "    Returns:\n",
    "        x: np.array[shape=(batch_size, datapoint_length)]\n",
    "        y: np.array[shape=(batch_size, datapoint_length)]\n",
    "    \"\"\"\n",
    "    \n",
    "    random.shuffle(data)\n",
    "    \n",
    "    endoftext_id = enc.encode('<|endoftext|>', allowed_special={\"<|endoftext|>\"})[0]\n",
    "    \n",
    "    block_size = 1024 # max_prompt_length + max_poem_length + 5\n",
    "    \n",
    "    x = np.ones(shape=(batch_size, block_size), dtype=np.int64) * endoftext_id\n",
    "    y = np.ones(shape=(batch_size, block_size), dtype=np.int64) * endoftext_id\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        concat_data = np.array(data[i][0] + data[i][1])\n",
    "        \n",
    "        x_len = min(len(concat_data), block_size)\n",
    "        y_len = min(len(concat_data)-1, block_size)\n",
    "        \n",
    "        x[i, :x_len]   = concat_data[:x_len]\n",
    "        y[i, :y_len] = concat_data[1:1+y_len]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e042b4",
   "metadata": {},
   "source": [
    "## Now, we train the model\n",
    "\n",
    "First things first, we set some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e960b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out'\n",
    "log_interval = 1\n",
    "eval_interval = 5\n",
    "eval_iters = 20\n",
    "\n",
    "gradient_accumulation_steps = 64 # used to simulate larger batch sizes\n",
    "batch_size = 1 # if gradient_accumulation_steps > 1, this is the micro-batch size\n",
    "dropout = 0.0\n",
    "\n",
    "learning_rate = 6e-5 # max learning rate\n",
    "max_iters = 40 # total number of training iterations\n",
    "warmup_iters = 2\n",
    "lr_decay_iters = 20\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "min_lr = 6e-6\n",
    "\n",
    "grad_clip = 1.0\n",
    "device = \"cuda\"\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dc34f",
   "metadata": {},
   "source": [
    "Then we load the GPT2-medium model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2699e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights\n",
      "loading weights from pretrained gpt: gpt2-medium\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahar/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 353.77M\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing from OpenAI GPT-2 weights\")\n",
    "\n",
    "override_args = dict(dropout=dropout)\n",
    "model = GPT.from_pretrained(\"gpt2-medium\", override_args)\n",
    "model_args = {}\n",
    "for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "    model_args[k] = getattr(model.config, k)\n",
    "\n",
    "model.to(device)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), \"cuda\")\n",
    "ctx = torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624f3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            the_data = train_data if split == 'train' else val_data\n",
    "            X, Y = get_batch(the_data, batch_size)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def get_batch(data, batch_size):\n",
    "    x, y = generate_batch(data, batch_size)\n",
    "    x_torch = torch.from_numpy(x).pin_memory().to(device, non_blocking=True)\n",
    "    y_torch = torch.from_numpy(y).pin_memory().to(device, non_blocking=True)\n",
    "    return x_torch, y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58be150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 6.5221, val loss 6.8813\n",
      "iter 0: loss 7.0547, time 36838.76ms, mfu -100.00%\n",
      "iter 1: loss 7.8221, time 31448.62ms, mfu -100.00%\n",
      "iter 2: loss 2.1180, time 31460.54ms, mfu -100.00%\n",
      "iter 3: loss 1.1617, time 31654.81ms, mfu -100.00%\n",
      "iter 4: loss 0.2709, time 31604.22ms, mfu -100.00%\n",
      "step 5: train loss 1.0766, val loss 0.7499\n",
      "saving checkpoint to out\n",
      "iter 5: loss 0.8109, time 42081.65ms, mfu 1.21%\n",
      "iter 6: loss 0.8773, time 31828.34ms, mfu 1.25%\n",
      "iter 7: loss 0.1865, time 31905.34ms, mfu 1.28%\n",
      "iter 8: loss 0.9060, time 31897.00ms, mfu 1.32%\n",
      "iter 9: loss 0.1690, time 31984.37ms, mfu 1.34%\n",
      "step 10: train loss 1.1920, val loss 1.2731\n",
      "iter 10: loss 2.4203, time 37511.66ms, mfu 1.34%\n",
      "iter 11: loss 1.6578, time 31979.43ms, mfu 1.37%\n",
      "iter 12: loss 0.5831, time 32016.03ms, mfu 1.39%\n",
      "iter 13: loss 0.1030, time 31982.63ms, mfu 1.41%\n",
      "iter 14: loss 3.9629, time 32067.66ms, mfu 1.43%\n",
      "step 15: train loss 1.1319, val loss 0.7877\n",
      "iter 15: loss 0.2679, time 37565.59ms, mfu 1.42%\n",
      "iter 16: loss 0.8444, time 31977.36ms, mfu 1.44%\n",
      "iter 17: loss 0.9147, time 31994.11ms, mfu 1.45%\n",
      "iter 18: loss 3.8546, time 32016.09ms, mfu 1.47%\n",
      "iter 19: loss 1.6863, time 32039.59ms, mfu 1.48%\n",
      "step 20: train loss 1.1854, val loss 0.9091\n",
      "iter 20: loss 0.8161, time 37657.17ms, mfu 1.47%\n",
      "iter 21: loss 1.1683, time 32013.84ms, mfu 1.48%\n",
      "iter 22: loss 1.3445, time 31910.49ms, mfu 1.49%\n",
      "iter 23: loss 1.0459, time 32008.23ms, mfu 1.50%\n",
      "iter 24: loss 1.2706, time 32020.81ms, mfu 1.51%\n",
      "step 25: train loss 0.9127, val loss 0.8233\n",
      "iter 25: loss 3.6373, time 37547.45ms, mfu 1.49%\n",
      "iter 26: loss 0.7177, time 31983.71ms, mfu 1.50%\n",
      "iter 27: loss 0.9097, time 32002.96ms, mfu 1.51%\n",
      "iter 28: loss 0.8511, time 32013.36ms, mfu 1.52%\n",
      "iter 29: loss 0.7645, time 32076.69ms, mfu 1.53%\n",
      "step 30: train loss 1.1117, val loss 1.0071\n",
      "iter 30: loss 0.7084, time 37754.72ms, mfu 1.51%\n",
      "iter 31: loss 1.3950, time 32186.40ms, mfu 1.52%\n",
      "iter 32: loss 1.3433, time 32034.12ms, mfu 1.52%\n",
      "iter 33: loss 1.1735, time 32080.00ms, mfu 1.53%\n",
      "iter 34: loss 0.9570, time 32003.68ms, mfu 1.54%\n",
      "step 35: train loss 1.2406, val loss 1.4028\n",
      "iter 35: loss 2.7419, time 37708.18ms, mfu 1.52%\n",
      "iter 36: loss 0.5065, time 32030.24ms, mfu 1.53%\n",
      "iter 37: loss 1.2335, time 32134.09ms, mfu 1.53%\n",
      "iter 38: loss 0.9164, time 32007.88ms, mfu 1.54%\n",
      "iter 39: loss 0.7524, time 31910.32ms, mfu 1.54%\n",
      "step 40: train loss 1.0602, val loss 0.7305\n",
      "saving checkpoint to out\n",
      "iter 40: loss 0.3212, time 43107.35ms, mfu 1.51%\n"
     ]
    }
   ],
   "source": [
    "X, Y = get_batch(train_data, batch_size)\n",
    "t0 = time.time()\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch(train_data, batch_size)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c43e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 1024)\n",
       "    (wpe): Embedding(1024, 1024)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-23): 24 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d238b",
   "metadata": {},
   "source": [
    "Now we sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89cf181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Renaissance Love poem written by WILLIAM SHAKESPEARE called \"Oh Romeo, My Love!\"\n",
      "\n",
      "\n",
      "\n",
      "Sweet Romeo, my love,\n",
      "The world is my music;\n",
      "But I am not you.\n",
      "\n",
      "\n",
      "I know enough of you to tell you what kind,\n",
      "To teach you that love does obey.\n",
      "\n",
      "What you want and know, you have;\n",
      "How little I know, you have only to know.\n"
     ]
    }
   ],
   "source": [
    "def truncate_output(id_list):\n",
    "    if id_list.index(50256) == -1:\n",
    "        return id_list\n",
    "    return id_list[:id_list.index(50256)]\n",
    "\n",
    "start_ids = encode(process_poem(\"WILLIAM SHAKESPEARE\", \"Oh Romeo, My Love!\", \n",
    "                                \"Renaissance\", \"Love\"))\n",
    "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "with torch.no_grad():\n",
    "    with ctx:\n",
    "        y = model.generate(x, 256, temperature=0.8, top_k=200)\n",
    "        print(enc.decode(truncate_output(y[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd8175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
