{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787b73c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import tiktoken\n",
    "import random\n",
    "from model import GPTConfig, GPT\n",
    "import time\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a361b3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efc7fea7550>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the random seed so we get the same results\n",
    "\n",
    "seed = 2845\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fee72b2",
   "metadata": {},
   "source": [
    "First, we need read the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3c6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('poetry.csv', newline='') as csvfile:\n",
    "    raw_data = list(csv.reader(csvfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c46c73",
   "metadata": {},
   "source": [
    "Then, we process it into a form that our GPT model can use. This is where we have to design our prompt and use soft prompting. Fill in the function `process_poem` to return a prompt based on the information we have about the poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e99b2b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_poem(author, name, age, poem_type):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        author: str\n",
    "            The author of the poem\n",
    "        name: str\n",
    "            The name of the poem\n",
    "        age: str\n",
    "            The \"age\" that the poem is from (either \"Renaissance\" or \"Modern\")\n",
    "                Note: each poem author will either be in the Renaissance\n",
    "                      or Modern age, meaning that this is redundant information\n",
    "        poem_type: str\n",
    "            The type of poem, will be one of these: \"Love\", \"Mythology & Folklore\", \"Nature\"\n",
    "    \n",
    "    Returns:\n",
    "        prompt: str\n",
    "            The prompt that we use for soft prompting\n",
    "    \"\"\"\n",
    "    \n",
    "    return \"Here is a \" + age + \" \" + poem_type + \" poem written by \" + author + ' called \"' + name + '\"\\n\\n'\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "encode = lambda x: enc.encode_ordinary(x)\n",
    "\n",
    "dataset = [(encode(process_poem(author, name, age, poem_type)), \n",
    "            encode(poem_content)) \n",
    "           for author, poem_content, name, age, poem_type in raw_data[1:]]\n",
    "\n",
    "max_prompt_length = max([len(poem[0]) for poem in dataset])\n",
    "max_poem_length   = max([len(poem[1]) for poem in dataset])\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "n = len(dataset)\n",
    "train_data = dataset[:int(n*0.9)]\n",
    "val_data = dataset[int(n*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a706ce",
   "metadata": {},
   "source": [
    "Next, we need to write code to sample from our dataset and generate a batch of data for us to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e309c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data: List[prompt, poem_content]\n",
    "            The dataset that we want to sample from (either training or validation)\n",
    "    Returns:\n",
    "        x: np.array[shape=(batch_size, datapoint_length)]\n",
    "        y: np.array[shape=(batch_size, datapoint_length)]\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_index = random.randrange(len(data))\n",
    "    \n",
    "    concat_data = np.array(data[sample_index][0] + data[sample_index][1])\n",
    "    \n",
    "    block_size = min(1025, len(concat_data))\n",
    "    \n",
    "    x = np.zeros(shape=(batch_size, block_size-1), dtype=np.int64)\n",
    "    y = np.zeros(shape=(batch_size, block_size-1), dtype=np.int64)\n",
    "\n",
    "    x = concat_data[:block_size-1]\n",
    "    y = concat_data[1:block_size]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e042b4",
   "metadata": {},
   "source": [
    "## Now, we train the model\n",
    "\n",
    "First things first, we set some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e960b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out'\n",
    "log_interval = 1\n",
    "eval_interval = 8\n",
    "eval_iters = 20\n",
    "\n",
    "batch_size = 64\n",
    "dropout = 0.1\n",
    "\n",
    "learning_rate = 2e-4 # max learning rate\n",
    "max_iters = 64 # total number of training iterations\n",
    "warmup_iters = 4\n",
    "lr_decay_iters = 32\n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "min_lr = 2e-6\n",
    "\n",
    "grad_clip = 1.0\n",
    "device = \"cuda\"\n",
    "\n",
    "config_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n",
    "config = {k: globals()[k] for k in config_keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115dc34f",
   "metadata": {},
   "source": [
    "Then we load the GPT2-medium model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2699e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing from OpenAI GPT-2 weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaharsandhaus/.conda/envs/182/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weights from pretrained gpt: gpt2-medium\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.1\n",
      "number of parameters: 353.77M\n",
      "using fused AdamW: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Initializing from OpenAI GPT-2 weights\")\n",
    "\n",
    "override_args = dict(dropout=dropout)\n",
    "model = GPT.from_pretrained(\"gpt2-medium\", override_args)\n",
    "model_args = {}\n",
    "for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "    model_args[k] = getattr(model.config, k)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=False)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), \"cuda\")\n",
    "ctx = torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8173516d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_model(author, name, age, poem_type):\n",
    "    start_ids = encode(process_poem(author, name, age, poem_type))\n",
    "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with ctx:\n",
    "            y = model.generate(x, 256, temperature=0.8, top_k=200)\n",
    "            return enc.decode(y[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1b655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a Renaissance Love poem written by HENRY VIII, KING OF ENGLAND called \"My Wife\"\n",
      "\n",
      "\n",
      "My Wife is a beautiful mistress with a sweet heart, The world is indeed a beautiful place, And it has always been a beautiful place.\n",
      "\n",
      "\n",
      "'My Wife' is based on the poem the Beatles wrote to Frank Sinatra about their love relationship.\n",
      "\n",
      "\n",
      "Here is a verse about Ethel Merman from the song \"Ethel Goes to the Prom\"\n",
      "\n",
      "\n",
      "Ethel Merman on the Prom song \"Ethel Merman Goes to the Prom\"\n",
      "\n",
      "\n",
      "Ethel Merman is an attractive and lovely woman, Ethel Merman is my wife and has been my wife since I was fourteen years old.\n",
      "\n",
      "\n",
      "You see the same Ethel Merman who wore her love bracelets that day, her lady dress and her high heels. She was quite a sight to behold at her inebriated best. It was hard for me to believe that those bracelets and high heels were my own, that they were bought by my father for me, his daughter, a little girl, that I was the child of Ethel Merman and Ethel Merman only, by my mother, Ethel Merman and my father and that Ethel Merman would let anyone else have them. So many beauty bracelets and high heels!\n",
      "\n",
      "\n",
      "Some\n"
     ]
    }
   ],
   "source": [
    "print(sample_model(\"HENRY VIII, KING OF ENGLAND\", \"My Wife\", \"Renaissance\", \"Love\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "624f3c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            the_data = train_data if split == 'train' else val_data\n",
    "            X, Y = get_batch(the_data)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# learning rate decay scheduler (cosine with warmup)\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "def get_batch(data):\n",
    "    x, y = generate_batch(data)\n",
    "    x_torch = torch.from_numpy(x).pin_memory().to(device, non_blocking=True)[None, ...]\n",
    "    y_torch = torch.from_numpy(y).pin_memory().to(device, non_blocking=True)[None, ...]\n",
    "    return x_torch, y_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 5.1421, val loss 5.0073\n",
      "iter 0: loss 3.6166, time 5401.06ms, mfu -100.00%\n",
      "iter 1: loss 4.7811, time 4519.43ms, mfu -100.00%\n",
      "iter 2: loss 4.9809, time 4738.86ms, mfu -100.00%\n",
      "iter 3: loss 4.9199, time 4662.13ms, mfu -100.00%\n",
      "iter 4: loss 4.6358, time 5459.18ms, mfu -100.00%\n",
      "iter 5: loss 4.9782, time 5064.82ms, mfu 10.06%\n",
      "iter 6: loss 4.4182, time 4724.18ms, mfu 10.13%\n",
      "iter 7: loss 3.9940, time 4880.24ms, mfu 10.16%\n",
      "step 8: train loss 3.3427, val loss 3.3909\n",
      "saving checkpoint to out\n",
      "iter 8: loss 3.6332, time 18804.79ms, mfu 9.41%\n",
      "iter 9: loss 4.7372, time 4717.05ms, mfu 9.55%\n",
      "iter 10: loss 3.8514, time 4317.95ms, mfu 9.78%\n",
      "iter 11: loss 3.8395, time 4674.32ms, mfu 9.89%\n",
      "iter 12: loss 4.1297, time 5591.74ms, mfu 9.81%\n",
      "iter 13: loss 3.6328, time 4492.32ms, mfu 9.96%\n",
      "iter 14: loss 1.4290, time 4631.42ms, mfu 10.07%\n",
      "iter 15: loss 4.3114, time 5453.52ms, mfu 9.99%\n",
      "step 16: train loss 2.7270, val loss 2.8175\n",
      "saving checkpoint to out\n",
      "iter 16: loss 3.8459, time 18932.84ms, mfu 9.26%\n",
      "iter 17: loss 1.4224, time 5195.83ms, mfu 9.32%\n",
      "iter 18: loss 3.1396, time 4571.74ms, mfu 9.50%\n",
      "iter 19: loss 2.7642, time 5318.26ms, mfu 9.51%\n",
      "iter 20: loss 3.1749, time 5205.12ms, mfu 9.53%\n",
      "iter 21: loss 2.9597, time 5161.02ms, mfu 9.57%\n",
      "iter 22: loss 3.3036, time 4618.97ms, mfu 9.71%\n",
      "iter 23: loss 3.3714, time 5185.76ms, mfu 9.72%\n",
      "step 24: train loss 2.9120, val loss 2.9169\n",
      "iter 24: loss 3.2776, time 5294.30ms, mfu 9.71%\n",
      "iter 25: loss 3.7003, time 4413.03ms, mfu 9.90%\n",
      "iter 26: loss 3.0329, time 4292.31ms, mfu 10.09%\n",
      "iter 27: loss 2.8194, time 4799.42ms, mfu 10.15%\n",
      "iter 28: loss 4.4411, time 4264.38ms, mfu 10.33%\n",
      "iter 29: loss 3.4714, time 4912.62ms, mfu 10.33%\n",
      "iter 30: loss 2.4945, time 4676.30ms, mfu 10.39%\n",
      "iter 31: loss 3.7711, time 4811.67ms, mfu 10.41%\n",
      "step 32: train loss 2.4942, val loss 2.6723\n",
      "saving checkpoint to out\n",
      "iter 32: loss 2.7562, time 18281.24ms, mfu 9.64%\n",
      "iter 33: loss 3.6578, time 4979.03ms, mfu 9.70%\n",
      "iter 34: loss 4.0651, time 5632.03ms, mfu 9.64%\n",
      "iter 35: loss 2.9723, time 4715.78ms, mfu 9.75%\n",
      "iter 36: loss 2.5075, time 5023.66ms, mfu 9.79%\n",
      "iter 37: loss 4.5419, time 5063.99ms, mfu 9.82%\n",
      "iter 38: loss 0.3739, time 5270.97ms, mfu 9.80%\n",
      "iter 39: loss 2.3220, time 5255.57ms, mfu 9.79%\n",
      "step 40: train loss 2.8847, val loss 2.8611\n",
      "iter 40: loss 2.9482, time 5302.82ms, mfu 9.77%\n",
      "iter 41: loss 3.5527, time 4579.73ms, mfu 9.91%\n",
      "iter 42: loss 3.1789, time 4777.47ms, mfu 9.98%\n",
      "iter 43: loss 1.0033, time 4577.59ms, mfu 10.10%\n",
      "iter 44: loss 3.3924, time 4486.61ms, mfu 10.22%\n",
      "iter 45: loss 3.8740, time 4385.09ms, mfu 10.36%\n",
      "iter 46: loss 2.8892, time 4123.27ms, mfu 10.56%\n",
      "iter 47: loss 2.7940, time 5096.53ms, mfu 10.50%\n",
      "step 48: train loss 2.3627, val loss 3.1641\n",
      "iter 48: loss 3.2292, time 6137.93ms, mfu 10.28%\n",
      "iter 49: loss 2.9820, time 4212.63ms, mfu 10.46%\n",
      "iter 50: loss 0.6770, time 4191.82ms, mfu 10.63%\n",
      "iter 51: loss 2.4226, time 4423.83ms, mfu 10.72%\n",
      "iter 52: loss 4.1544, time 4472.85ms, mfu 10.79%\n",
      "iter 53: loss 3.5238, time 4847.46ms, mfu 10.76%\n",
      "iter 54: loss 2.4188, time 5026.46ms, mfu 10.70%\n",
      "iter 55: loss 3.5245, time 4670.47ms, mfu 10.72%\n",
      "step 56: train loss 2.3790, val loss 2.8749\n",
      "iter 56: loss 2.9204, time 4790.66ms, mfu 10.71%\n",
      "iter 57: loss 3.4905, time 4379.28ms, mfu 10.80%\n",
      "iter 58: loss 2.5272, time 4904.53ms, mfu 10.76%\n",
      "iter 59: loss 3.5849, time 5020.71ms, mfu 10.70%\n",
      "iter 60: loss 2.3142, time 3854.41ms, mfu 10.95%\n",
      "iter 61: loss 3.1864, time 4970.98ms, mfu 10.88%\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "X, Y = get_batch(train_data)\n",
    "t0 = time.time()\n",
    "local_iter_num = 0\n",
    "running_mfu = -1.0\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "while True:\n",
    "    # determine and set the learning rate for this iteration\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    if iter_num % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
    "\n",
    "    # forward backward update, with optional gradient accumulation to simulate larger batch size\n",
    "    # and using the GradScaler if data type is float16\n",
    "    for micro_step in range(batch_size):\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / batch_size # scale the loss to account for gradient accumulation\n",
    "        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n",
    "        X, Y = get_batch(train_data)\n",
    "        # backward pass, with gradient scaling if training in fp16\n",
    "        scaler.scale(loss).backward()\n",
    "    # clip the gradient\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    # step the optimizer and scaler if training in fp16\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    # flush the gradients as soon as we can, no need for this memory anymore\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # timing and logging\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0:\n",
    "        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n",
    "        lossf = loss.item() * batch_size\n",
    "        if local_iter_num >= 5: # let the training loop settle a bit\n",
    "            mfu = model.estimate_mfu(batch_size, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    # termination conditions\n",
    "    if iter_num > max_iters:\n",
    "        break\n",
    "    \n",
    "model.eval()\n",
    "None # to avoid printing the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938d238b",
   "metadata": {},
   "source": [
    "Now we sample from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cf181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_model(\"HENRY VIII, KING OF ENGLAND\", \"My Wife\", \"Renaissance\", \"Love\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_model(\"HENRY VIII, KING OF ENGLAND\", \"My Wife\", \"Renaissance\", \"Love\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ceab59",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_model(\"HENRY VIII, KING OF ENGLAND\", \"My Wife\", \"Renaissance\", \"Love\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837cdea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
